----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------sent
env.action_space is [MultiDiscrete2, Discrete(5), Discrete(5), Discrete(5), Discrete(5), Discrete(5)] 
env.observation_space is [Box(34,), Box(34,), Box(34,), Box(34,), Box(28,), Box(28,)] 
obs_shape_n is [(34,), (34,), (34,), (34,), (28,), (28,)] 
Using good policy maddpg and adv policy maddpg
[Discrete(34), Discrete(34), Discrete(34), Discrete(34), Discrete(28), Discrete(28)]
Using noise policy maddpg
There is 6 adversaries
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -31.143327896457382, agent episode reward: [0.7969381349208206, 0.8636167998719122, 0.9169408450789389, 0.925977505689379, -16.301651746302113, -18.345149435716326], time: 1240.885
steps: 49975, episodes: 2000, mean episode reward: -17.7921097865983, agent episode reward: [2.769672573196778, 2.0858054830933, 2.5667868121738198, 2.3232973973737145, -18.165624666007083, -9.372047386428832], time: 1945.484
steps: 74975, episodes: 3000, mean episode reward: 3.9642187780990006, agent episode reward: [3.661698922960286, 3.178910858374401, 3.4533340381200635, 3.463362696989391, -6.094351602682552, -3.698736135662589], time: 1941.805
steps: 99975, episodes: 4000, mean episode reward: -2.67143362140707, agent episode reward: [4.256682133099115, 3.968069703473234, 3.9553389990601264, 4.321343269630638, -11.75674731024662, -7.416120416423565], time: 1946.336
steps: 124975, episodes: 5000, mean episode reward: 1.638000095105546, agent episode reward: [4.873907563276136, 4.680712184812998, 4.732119518573871, 5.035518916904206, -11.296623017750349, -6.387635070711316], time: 1947.266
steps: 149975, episodes: 6000, mean episode reward: 4.924379947001474, agent episode reward: [4.945922870261256, 4.874705839617742, 5.154709395255124, 5.078244687672399, -8.986284602949617, -6.142918242855429], time: 1946.61
steps: 174975, episodes: 7000, mean episode reward: 7.879517460605321, agent episode reward: [4.780726910481944, 4.653074151700534, 5.03899195839294, 5.021647050141106, -6.148947288193932, -5.465975321917269], time: 1946.68
steps: 199975, episodes: 8000, mean episode reward: 13.81972356655958, agent episode reward: [5.7801549891121615, 5.365111785272903, 5.918573361387412, 5.8967274344865785, -4.342464650188976, -4.798379353510498], time: 1943.596
steps: 224975, episodes: 9000, mean episode reward: 16.434806780666506, agent episode reward: [7.1640530587116045, 7.036874558925438, 7.219986859962254, 7.102955353704207, -5.2575981148689115, -6.831464935768088], time: 1920.667
steps: 249975, episodes: 10000, mean episode reward: 19.61094671040335, agent episode reward: [8.092369431758579, 8.073390554699888, 8.144904063529621, 7.955663998992269, -5.646085072080648, -7.009296266496359], time: 1474.19
steps: 274975, episodes: 11000, mean episode reward: 17.147711614348342, agent episode reward: [7.787978085061311, 7.6870932264976854, 7.768226360872044, 7.549590046802226, -6.132067006394406, -7.513109098490517], time: 1105.946
steps: 299975, episodes: 12000, mean episode reward: 19.67516022744333, agent episode reward: [8.46220957853999, 8.509955584070934, 8.491972589660783, 8.256340800008399, -7.214069654042568, -6.831248670794205], time: 972.46
steps: 324975, episodes: 13000, mean episode reward: 20.993610395569927, agent episode reward: [8.549403533124476, 8.466367041320897, 8.420174500511312, 8.437583485350906, -6.912604145703136, -5.967314019034522], time: 972.888
steps: 349975, episodes: 14000, mean episode reward: 20.917011751414034, agent episode reward: [8.747003700553977, 8.6448832420984, 8.560137836810476, 8.68548770951378, -7.234362848394336, -6.486137889168267], time: 972.428
steps: 374975, episodes: 15000, mean episode reward: 19.224015342014187, agent episode reward: [8.361980094312079, 8.25754186734137, 8.252359874906546, 8.390470793794934, -8.013169457520656, -6.025167830820087], time: 976.961
steps: 399975, episodes: 16000, mean episode reward: 22.572906182634327, agent episode reward: [9.365920934852419, 9.28961463767387, 9.31780646788902, 9.489943747827917, -7.982566084090533, -6.90781352151836], time: 801.055
steps: 424975, episodes: 17000, mean episode reward: 20.30774231455964, agent episode reward: [9.238938230914833, 9.290573105087578, 9.16386908588514, 9.321368058539523, -9.44836668898205, -7.258639476885382], time: 571.74
steps: 449975, episodes: 18000, mean episode reward: 19.054070602878205, agent episode reward: [8.81306829685739, 8.83157777820259, 8.713092653756998, 8.9146986716585, -9.426613321315946, -6.79175347628133], time: 573.093
steps: 474975, episodes: 19000, mean episode reward: 19.862491675195276, agent episode reward: [9.02468068993069, 9.00381975609791, 8.928641363238231, 9.07257721124746, -9.302096639477305, -6.865130705841709], time: 573.485
steps: 499975, episodes: 20000, mean episode reward: 22.297413145461316, agent episode reward: [9.706342695505803, 9.655292187875613, 9.678829319088248, 9.717613104581917, -9.639793800972257, -6.82087036061801], time: 571.554
steps: 524975, episodes: 21000, mean episode reward: 22.124413999231315, agent episode reward: [9.59959424304699, 9.470870315890847, 9.465255498840172, 9.588363420723296, -9.297138309554159, -6.702531169715826], time: 572.664
steps: 549975, episodes: 22000, mean episode reward: 19.276256490151088, agent episode reward: [8.973738916250921, 8.837635307033247, 8.81870320057636, 8.959187111148562, -10.021615248289008, -6.291392796568991], time: 574.369
steps: 574975, episodes: 23000, mean episode reward: 14.799127842128833, agent episode reward: [7.4122526228817085, 7.267599853735342, 7.292930004558716, 7.348633829608892, -7.134043714443314, -7.388244754212514], time: 577.218
steps: 599975, episodes: 24000, mean episode reward: 19.856719592295338, agent episode reward: [8.288774411801844, 8.204537769703698, 8.308450446176908, 8.209790172764203, -6.288142151569203, -6.866691056582111], time: 576.801
steps: 624975, episodes: 25000, mean episode reward: 22.33809209948455, agent episode reward: [9.073200957260877, 9.080417241147702, 9.154254573248101, 9.174001822312526, -6.813804065733288, -7.3299784287513665], time: 575.497
steps: 649975, episodes: 26000, mean episode reward: 18.84973564632165, agent episode reward: [8.755713311637718, 8.725336994435693, 8.811032990734788, 8.768743544445751, -7.330704867933037, -8.880386326999258], time: 577.537
steps: 674975, episodes: 27000, mean episode reward: 21.491301589279136, agent episode reward: [9.09646110047303, 9.055440616525031, 9.053128763230594, 9.004611405357752, -8.0429428406259, -6.675397455681372], time: 576.702
steps: 699975, episodes: 28000, mean episode reward: 20.113754656229176, agent episode reward: [8.727507980635691, 8.62240662706564, 8.695812191344, 8.66900380795556, -7.45780031159088, -7.143175639180839], time: 577.943
steps: 724975, episodes: 29000, mean episode reward: 19.661102872905325, agent episode reward: [8.161399606881165, 8.104125920483042, 8.138385833155066, 8.193862595153883, -6.683941708724701, -6.252729374043132], time: 576.607
steps: 749975, episodes: 30000, mean episode reward: 18.466859906930974, agent episode reward: [7.939215388561463, 7.872564391071467, 7.89475843191182, 7.883548864365932, -7.633641900856366, -5.48958526812334], time: 578.331
steps: 774975, episodes: 31000, mean episode reward: 19.199185429237687, agent episode reward: [8.356931371939964, 8.188446307257225, 8.311711908971292, 8.290985236733482, -6.902083616993984, -7.046805778670293], time: 575.145
steps: 799975, episodes: 32000, mean episode reward: 18.809237841222007, agent episode reward: [8.35496902606185, 8.236323750171683, 8.277759440540642, 8.191772776433908, -6.761287721209251, -7.49029943077683], time: 577.561
steps: 824975, episodes: 33000, mean episode reward: 14.022476381072615, agent episode reward: [7.250012115595981, 7.004409435465938, 7.20802557101477, 7.059841793067682, -7.389300403244661, -7.110512130827096], time: 554.903
steps: 849975, episodes: 34000, mean episode reward: 15.733538434799666, agent episode reward: [7.230559479844503, 7.078031594011994, 7.134451351533595, 6.922173877376907, -6.513307550531365, -6.1183703174359705], time: 272.105
steps: 874975, episodes: 35000, mean episode reward: 13.479398086024151, agent episode reward: [7.1940859425295915, 6.9733803894136495, 7.059495220803327, 6.963758814001323, -8.303848347113675, -6.407473933610064], time: 275.704
steps: 899975, episodes: 36000, mean episode reward: 12.591369421121163, agent episode reward: [7.211655804892727, 7.003538389735379, 7.071563922231174, 7.037369976535815, -8.976588550696537, -6.756170121577397], time: 272.561
steps: 924975, episodes: 37000, mean episode reward: 13.254928898241559, agent episode reward: [7.0215558036009265, 6.891389664386132, 6.971736038436472, 6.937977165593422, -6.993898439077473, -7.573831334697921], time: 272.554
steps: 949975, episodes: 38000, mean episode reward: 9.576324633810419, agent episode reward: [6.57779183104047, 6.381492071471443, 6.448133381988729, 6.467057612430143, -9.305446795702375, -6.992703467417987], time: 272.553
steps: 974975, episodes: 39000, mean episode reward: 11.239392204603623, agent episode reward: [6.692467578827376, 6.739588172131805, 6.580078990690078, 6.600599578939638, -8.397988693404015, -6.975353422581258], time: 272.536
steps: 999975, episodes: 40000, mean episode reward: 10.141485738023926, agent episode reward: [6.722494527316762, 6.819444315218036, 6.7692553675720735, 6.727083725951928, -9.924643070883743, -6.972149127151131], time: 273.032
steps: 1024975, episodes: 41000, mean episode reward: 11.23017785535911, agent episode reward: [6.990174895282891, 6.879803951235154, 6.795435027196262, 6.911010986327645, -8.837445684281585, -7.508801320401256], time: 273.981
steps: 1049975, episodes: 42000, mean episode reward: 12.346356747416161, agent episode reward: [7.212786902124778, 7.182405351865159, 7.065114249071545, 7.206250483471831, -8.560441483328846, -7.7597587557883045], time: 276.075
steps: 1074975, episodes: 43000, mean episode reward: 13.440421421161199, agent episode reward: [7.181884777305522, 7.038884600671883, 7.009480946416217, 7.119813164150632, -8.143211343643282, -6.766430723739774], time: 274.586
steps: 1099975, episodes: 44000, mean episode reward: 12.453689320086587, agent episode reward: [6.884597020204766, 6.762950218023833, 6.774966473564284, 6.81331160793518, -8.326989651961412, -6.4551463476800635], time: 274.747
steps: 1124975, episodes: 45000, mean episode reward: 11.136364554911518, agent episode reward: [6.606752280700403, 6.532093861266755, 6.50079961134643, 6.540205451731795, -8.607493008204335, -6.43599364192953], time: 274.73
steps: 1149975, episodes: 46000, mean episode reward: 10.71188198449477, agent episode reward: [7.262969289951338, 7.122828822741335, 7.077819488932193, 7.108832846656837, -8.812468936183716, -9.048099527603217], time: 275.676
steps: 1174975, episodes: 47000, mean episode reward: 8.638286992802652, agent episode reward: [5.89485112052141, 5.801133519683983, 5.787073137770322, 5.82923528796179, -6.961036326846119, -7.712969746288734], time: 275.657
steps: 1199975, episodes: 48000, mean episode reward: 12.467783304739172, agent episode reward: [6.826697348867329, 6.733703531851957, 6.749584220670241, 6.811683856356658, -6.878832904700116, -7.775052748306896], time: 276.094
steps: 1224975, episodes: 49000, mean episode reward: 12.987737337683939, agent episode reward: [7.196474601230891, 7.042006128930638, 6.9466276437689665, 7.057227230545352, -8.458399648196005, -6.796198618595905], time: 276.695
steps: 1249975, episodes: 50000, mean episode reward: 12.954916853327893, agent episode reward: [6.759925873367922, 6.604977147441297, 6.651497471627253, 6.7245274021016215, -7.576278037992906, -6.209733003217295], time: 276.026
steps: 1274975, episodes: 51000, mean episode reward: 14.814719154371778, agent episode reward: [6.887080056557798, 6.825271393025192, 6.798778255207649, 6.855106874797271, -5.584632661254712, -6.9668847639614215], time: 271.174
steps: 1299975, episodes: 52000, mean episode reward: 13.052471129338603, agent episode reward: [6.356338060137854, 6.296888626331908, 6.22299505040572, 6.240859939535964, -6.276344662727841, -5.788265884345001], time: 271.696
steps: 1324975, episodes: 53000, mean episode reward: 12.016002013256784, agent episode reward: [6.429155797806434, 6.421664941834861, 6.340554569911213, 6.401137739133706, -7.242772651271905, -6.333738384157523], time: 271.598
steps: 1349975, episodes: 54000, mean episode reward: 13.333203846593559, agent episode reward: [6.62441825844171, 6.502760750457284, 6.484174843163768, 6.524447811961528, -6.618337439174107, -6.1842603782566234], time: 272.093
steps: 1374975, episodes: 55000, mean episode reward: 14.57542980568393, agent episode reward: [6.881958367962292, 6.80174315662058, 6.803433479120835, 6.78078032501172, -5.863514000783882, -6.828971522247614], time: 273.28
steps: 1399975, episodes: 56000, mean episode reward: 12.068335019732576, agent episode reward: [6.1815714430115065, 6.254430944842565, 6.1510216197219965, 6.162331913361875, -5.642153840881785, -7.0388670603235814], time: 276.386
steps: 1424975, episodes: 57000, mean episode reward: 7.439515322924174, agent episode reward: [5.304731458955934, 5.432482603039666, 5.3559523626168755, 5.356148063524188, -7.884424707835786, -6.125374457376704], time: 276.718
steps: 1449975, episodes: 58000, mean episode reward: 9.770912135544636, agent episode reward: [5.491126672326362, 5.436652801782412, 5.524935225076742, 5.397170333127547, -5.727252630470987, -6.351720266297438], time: 277.207
steps: 1474975, episodes: 59000, mean episode reward: 8.569722946502425, agent episode reward: [5.355913705454861, 5.424504077529314, 5.409279717287012, 5.37854226337347, -7.383741920460618, -5.614774896681614], time: 276.906
steps: 1499975, episodes: 60000, mean episode reward: 9.47216862249493, agent episode reward: [5.683097134258639, 5.707824334219619, 5.590440967476727, 5.607478278567021, -6.680463545768032, -6.436208546259047], time: 277.024
...Finished total of 60001 episodes.
----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------end
