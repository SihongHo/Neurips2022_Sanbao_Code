env.action_space is [Discrete(5), Discrete(5), Discrete(5)] 
env.observation_space is [Box(8,), Box(10,), Box(10,)] 
obs_shape_n is [(8,), (10,), (10,)] 
Using good policy maddpg and adv policy maddpg
[Discrete(8), Discrete(10), Discrete(10)]
Using noise policy maddpg
There is 3 adversaries
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -22.57091419752994, agent episode reward: [-36.30327336255764, 6.866179582513847, 6.866179582513847], time: 59.679
steps: 49975, episodes: 2000, mean episode reward: -21.177598947170235, agent episode reward: [-30.350016134687554, 4.58620859375866, 4.58620859375866], time: 82.25
steps: 74975, episodes: 3000, mean episode reward: -12.033389550739066, agent episode reward: [-24.42974700957851, 6.19817872941972, 6.19817872941972], time: 80.565
steps: 99975, episodes: 4000, mean episode reward: -10.954872361089873, agent episode reward: [-23.827971194189804, 6.4365494165499655, 6.4365494165499655], time: 81.943
steps: 124975, episodes: 5000, mean episode reward: -12.280296247831858, agent episode reward: [-24.732009402241054, 6.225856577204598, 6.225856577204598], time: 81.518
steps: 149975, episodes: 6000, mean episode reward: -11.685681798992722, agent episode reward: [-26.13457027673606, 7.22444423887167, 7.22444423887167], time: 83.77
steps: 174975, episodes: 7000, mean episode reward: -12.202317325724689, agent episode reward: [-24.0670855555145, 5.932384114894903, 5.932384114894903], time: 81.205
steps: 199975, episodes: 8000, mean episode reward: -12.469615173441195, agent episode reward: [-24.828668338407716, 6.179526582483261, 6.179526582483261], time: 81.097
steps: 224975, episodes: 9000, mean episode reward: -10.654522823320196, agent episode reward: [-24.259494846603054, 6.80248601164143, 6.80248601164143], time: 81.957
steps: 249975, episodes: 10000, mean episode reward: -11.790770624411845, agent episode reward: [-25.352000277361366, 6.78061482647476, 6.78061482647476], time: 82.544
steps: 274975, episodes: 11000, mean episode reward: -12.551848198323372, agent episode reward: [-23.805139917265734, 5.626645859471181, 5.626645859471181], time: 82.72
steps: 299975, episodes: 12000, mean episode reward: -11.493012337822023, agent episode reward: [-22.98273920162594, 5.744863431901957, 5.744863431901957], time: 82.155
steps: 324975, episodes: 13000, mean episode reward: -11.644594811092858, agent episode reward: [-23.05581499332038, 5.70561009111376, 5.70561009111376], time: 82.032
steps: 349975, episodes: 14000, mean episode reward: -11.921629803158822, agent episode reward: [-22.20730549409894, 5.142837845470059, 5.142837845470059], time: 81.803
steps: 374975, episodes: 15000, mean episode reward: -13.514779439736879, agent episode reward: [-22.24303127372705, 4.364125916995085, 4.364125916995085], time: 83.572
steps: 399975, episodes: 16000, mean episode reward: -13.875214038514606, agent episode reward: [-22.659964175763168, 4.392375068624282, 4.392375068624282], time: 82.816
steps: 424975, episodes: 17000, mean episode reward: -12.503450603382275, agent episode reward: [-21.277254095440927, 4.386901746029325, 4.386901746029325], time: 82.703
steps: 449975, episodes: 18000, mean episode reward: -13.476257464683222, agent episode reward: [-20.26480652543263, 3.3942745303747044, 3.3942745303747044], time: 82.713
steps: 474975, episodes: 19000, mean episode reward: -14.49441155139624, agent episode reward: [-22.674209540916365, 4.0898989947600635, 4.0898989947600635], time: 82.205
steps: 499975, episodes: 20000, mean episode reward: -13.07962489914609, agent episode reward: [-23.1824639701269, 5.051419535490406, 5.051419535490406], time: 82.88
steps: 524975, episodes: 21000, mean episode reward: -12.995990562525622, agent episode reward: [-21.540842294395013, 4.272425865934694, 4.272425865934694], time: 82.899
steps: 549975, episodes: 22000, mean episode reward: -13.10749645794848, agent episode reward: [-22.18510942518921, 4.538806483620364, 4.538806483620364], time: 83.715
steps: 574975, episodes: 23000, mean episode reward: -13.46261550785752, agent episode reward: [-20.593876759241788, 3.5656306256921337, 3.5656306256921337], time: 84.333
steps: 599975, episodes: 24000, mean episode reward: -13.84082346399665, agent episode reward: [-20.877711316889982, 3.5184439264466665, 3.5184439264466665], time: 83.366
steps: 624975, episodes: 25000, mean episode reward: -12.968435337168613, agent episode reward: [-20.86963464676922, 3.9505996548003046, 3.9505996548003046], time: 84.064
steps: 649975, episodes: 26000, mean episode reward: -13.597543748022705, agent episode reward: [-20.429603803832947, 3.4160300279051214, 3.4160300279051214], time: 82.76
steps: 674975, episodes: 27000, mean episode reward: -14.568763366575055, agent episode reward: [-20.780763483204915, 3.1060000583149305, 3.1060000583149305], time: 82.436
steps: 699975, episodes: 28000, mean episode reward: -13.87143991001144, agent episode reward: [-20.33172802620796, 3.230144058098258, 3.230144058098258], time: 83.443
steps: 724975, episodes: 29000, mean episode reward: -13.023602322418203, agent episode reward: [-20.085084009410792, 3.5307408434962935, 3.5307408434962935], time: 83.268
steps: 749975, episodes: 30000, mean episode reward: -11.669417704512313, agent episode reward: [-20.392426244833327, 4.361504270160509, 4.361504270160509], time: 83.117
steps: 774975, episodes: 31000, mean episode reward: -13.537208229133576, agent episode reward: [-20.672934602512047, 3.5678631866892356, 3.5678631866892356], time: 83.248
steps: 799975, episodes: 32000, mean episode reward: -12.45438071258686, agent episode reward: [-19.967174780764566, 3.7563970340888537, 3.7563970340888537], time: 82.715
steps: 824975, episodes: 33000, mean episode reward: -13.723631540403195, agent episode reward: [-20.348789722997676, 3.3125790912972413, 3.3125790912972413], time: 83.469
steps: 849975, episodes: 34000, mean episode reward: -13.951570698886508, agent episode reward: [-21.373332386198737, 3.7108808436561143, 3.7108808436561143], time: 83.303
steps: 874975, episodes: 35000, mean episode reward: -12.869479489298056, agent episode reward: [-20.404811841293103, 3.767666175997522, 3.767666175997522], time: 83.107
steps: 899975, episodes: 36000, mean episode reward: -14.415523427354696, agent episode reward: [-21.208824647074522, 3.396650609859914, 3.396650609859914], time: 82.478
steps: 924975, episodes: 37000, mean episode reward: -14.060405535703957, agent episode reward: [-21.318889533661576, 3.6292419989788085, 3.6292419989788085], time: 83.486
steps: 949975, episodes: 38000, mean episode reward: -15.352460994457362, agent episode reward: [-20.8901281294468, 2.7688335674947173, 2.7688335674947173], time: 82.786
steps: 974975, episodes: 39000, mean episode reward: -14.488196107652987, agent episode reward: [-20.406709625426146, 2.959256758886579, 2.959256758886579], time: 83.056
steps: 999975, episodes: 40000, mean episode reward: -13.521901929137293, agent episode reward: [-20.7195409157919, 3.5988194933273023, 3.5988194933273023], time: 84.065
steps: 1024975, episodes: 41000, mean episode reward: -13.661627122468174, agent episode reward: [-19.84845667660119, 3.0934147770665095, 3.0934147770665095], time: 84.831
steps: 1049975, episodes: 42000, mean episode reward: -13.829980506575852, agent episode reward: [-21.636251374002466, 3.903135433713309, 3.903135433713309], time: 84.018
steps: 1074975, episodes: 43000, mean episode reward: -14.109222193680896, agent episode reward: [-20.23236376109275, 3.0615707837059283, 3.0615707837059283], time: 82.915
steps: 1099975, episodes: 44000, mean episode reward: -13.100593095153396, agent episode reward: [-20.195104793290035, 3.547255849068322, 3.547255849068322], time: 83.51
steps: 1124975, episodes: 45000, mean episode reward: -13.555394866436774, agent episode reward: [-21.40268809243827, 3.9236466130007495, 3.9236466130007495], time: 83.602
steps: 1149975, episodes: 46000, mean episode reward: -12.98181499395318, agent episode reward: [-20.403438009182057, 3.7108115076144386, 3.7108115076144386], time: 82.959
steps: 1174975, episodes: 47000, mean episode reward: -13.497381319028369, agent episode reward: [-20.847467885965067, 3.6750432834683466, 3.6750432834683466], time: 82.506
steps: 1199975, episodes: 48000, mean episode reward: -12.196728466299236, agent episode reward: [-20.649243975064774, 4.226257754382768, 4.226257754382768], time: 82.465
steps: 1224975, episodes: 49000, mean episode reward: -12.209587524774083, agent episode reward: [-21.27194142713021, 4.531176951178064, 4.531176951178064], time: 83.318
steps: 1249975, episodes: 50000, mean episode reward: -11.636358949654625, agent episode reward: [-20.624961840943012, 4.4943014456441945, 4.4943014456441945], time: 82.617
steps: 1274975, episodes: 51000, mean episode reward: -11.788326506213329, agent episode reward: [-21.345868582654685, 4.778771038220678, 4.778771038220678], time: 82.97
steps: 1299975, episodes: 52000, mean episode reward: -10.958305601057894, agent episode reward: [-21.662453579887398, 5.352073989414752, 5.352073989414752], time: 83.623
steps: 1324975, episodes: 53000, mean episode reward: -11.810859262531576, agent episode reward: [-20.957907634614777, 4.573524186041599, 4.573524186041599], time: 82.804
steps: 1349975, episodes: 54000, mean episode reward: -12.463035888121457, agent episode reward: [-21.06372806620375, 4.30034608904115, 4.30034608904115], time: 83.187
steps: 1374975, episodes: 55000, mean episode reward: -12.036184584543488, agent episode reward: [-21.271336326412754, 4.617575870934636, 4.617575870934636], time: 82.771
steps: 1399975, episodes: 56000, mean episode reward: -12.011756618807903, agent episode reward: [-20.95773188402948, 4.472987632610789, 4.472987632610789], time: 82.091
steps: 1424975, episodes: 57000, mean episode reward: -11.601722039401821, agent episode reward: [-21.49647309340194, 4.94737552700006, 4.94737552700006], time: 82.488
steps: 1449975, episodes: 58000, mean episode reward: -12.424291743175159, agent episode reward: [-21.09072687363856, 4.3332175652317, 4.3332175652317], time: 83.308
steps: 1474975, episodes: 59000, mean episode reward: -12.203593840611179, agent episode reward: [-21.451611685516095, 4.6240089224524565, 4.6240089224524565], time: 83.792
steps: 1499975, episodes: 60000, mean episode reward: -12.611214051013732, agent episode reward: [-21.863303195435517, 4.626044572210892, 4.626044572210892], time: 82.992
...Finished total of 60001 episodes.
