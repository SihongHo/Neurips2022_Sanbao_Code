env.action_space is [Discrete(4), Discrete(4), Discrete(4)] 
env.observation_space is [Box(4,), Box(8,), Box(8,)] 
obs_shape_n is [(4,), (8,), (8,)] 
Using good policy maddpg and adv policy maddpg
[Discrete(4), Discrete(8), Discrete(8)]
Using noise policy maddpg
There is 3 adversaries
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.524272671464313, agent episode reward: [-25.151941150860715, -0.6861657603017987, -0.6861657603017987], time: 93.14
steps: 49975, episodes: 2000, mean episode reward: -20.298050207031444, agent episode reward: [-18.587929657067892, -0.8550602749817756, -0.8550602749817756], time: 120.307
steps: 74975, episodes: 3000, mean episode reward: -20.022764931705076, agent episode reward: [-17.600050037049368, -1.2113574473278528, -1.2113574473278528], time: 106.868
steps: 99975, episodes: 4000, mean episode reward: -22.577839249116884, agent episode reward: [-20.4499330548136, -1.0639530971516418, -1.0639530971516418], time: 107.135
steps: 124975, episodes: 5000, mean episode reward: -24.774437760434864, agent episode reward: [-21.30569616895874, -1.7343707957380605, -1.7343707957380605], time: 105.743
steps: 149975, episodes: 6000, mean episode reward: -24.901599877575435, agent episode reward: [-21.64110600976335, -1.6302469339060404, -1.6302469339060404], time: 106.245
steps: 174975, episodes: 7000, mean episode reward: -25.492088308802703, agent episode reward: [-21.111849211360408, -2.1901195487211487, -2.1901195487211487], time: 106.295
steps: 199975, episodes: 8000, mean episode reward: -24.270591042877967, agent episode reward: [-21.045804398131793, -1.6123933223730869, -1.6123933223730869], time: 107.218
steps: 224975, episodes: 9000, mean episode reward: -25.427499884326863, agent episode reward: [-20.86613264799184, -2.2806836181675116, -2.2806836181675116], time: 106.141
steps: 249975, episodes: 10000, mean episode reward: -25.506815910689532, agent episode reward: [-20.807984691967395, -2.3494156093610683, -2.3494156093610683], time: 106.917
steps: 274975, episodes: 11000, mean episode reward: -25.6671766478745, agent episode reward: [-20.85650268414834, -2.405336981863079, -2.405336981863079], time: 106.466
steps: 299975, episodes: 12000, mean episode reward: -25.06267703255964, agent episode reward: [-20.974891884655246, -2.043892573952198, -2.043892573952198], time: 106.623
steps: 324975, episodes: 13000, mean episode reward: -25.4813985087142, agent episode reward: [-20.79301177874234, -2.3441933649859306, -2.3441933649859306], time: 105.488
steps: 349975, episodes: 14000, mean episode reward: -24.8742530655935, agent episode reward: [-20.891426578998516, -1.9914132432974936, -1.9914132432974936], time: 106.972
steps: 374975, episodes: 15000, mean episode reward: -24.89960615583578, agent episode reward: [-21.18640421139694, -1.8566009722194179, -1.8566009722194179], time: 106.494
steps: 399975, episodes: 16000, mean episode reward: -25.44014517936261, agent episode reward: [-20.73388755034487, -2.353128814508868, -2.353128814508868], time: 106.197
steps: 424975, episodes: 17000, mean episode reward: -24.861683084989934, agent episode reward: [-21.351403601779435, -1.7551397416052514, -1.7551397416052514], time: 106.099
steps: 449975, episodes: 18000, mean episode reward: -25.348188354544867, agent episode reward: [-20.888918790587944, -2.2296347819784597, -2.2296347819784597], time: 106.716
steps: 474975, episodes: 19000, mean episode reward: -25.073353082169948, agent episode reward: [-21.217568980068503, -1.9278920510507223, -1.9278920510507223], time: 106.761
steps: 499975, episodes: 20000, mean episode reward: -25.894584397050075, agent episode reward: [-20.698152568866142, -2.5982159140919685, -2.5982159140919685], time: 96.502
steps: 524975, episodes: 21000, mean episode reward: -25.146255606018844, agent episode reward: [-21.072905921001038, -2.0366748425089027, -2.0366748425089027], time: 80.773
steps: 549975, episodes: 22000, mean episode reward: -23.68536888392162, agent episode reward: [-21.75347914461755, -0.9659448696520353, -0.9659448696520353], time: 80.953
steps: 574975, episodes: 23000, mean episode reward: -25.785834766624657, agent episode reward: [-20.885618413199037, -2.45010817671281, -2.45010817671281], time: 80.828
steps: 599975, episodes: 24000, mean episode reward: -24.21287617366509, agent episode reward: [-21.653650283640637, -1.27961294501223, -1.27961294501223], time: 84.433
steps: 624975, episodes: 25000, mean episode reward: -24.607915850699268, agent episode reward: [-21.444409011966577, -1.581753419366344, -1.581753419366344], time: 109.046
steps: 649975, episodes: 26000, mean episode reward: -24.207929685901238, agent episode reward: [-21.3182990831257, -1.4448153013877683, -1.4448153013877683], time: 107.765
steps: 674975, episodes: 27000, mean episode reward: -23.807191359295786, agent episode reward: [-21.49892404357191, -1.1541336578619346, -1.1541336578619346], time: 107.887
steps: 699975, episodes: 28000, mean episode reward: -25.163940389707186, agent episode reward: [-21.379655231854183, -1.8921425789265027, -1.8921425789265027], time: 107.82
steps: 724975, episodes: 29000, mean episode reward: -24.399165885809097, agent episode reward: [-21.711049892907138, -1.3440579964509778, -1.3440579964509778], time: 107.243
steps: 749975, episodes: 30000, mean episode reward: -24.272292228988285, agent episode reward: [-21.406494347918787, -1.4328989405347496, -1.4328989405347496], time: 107.589
steps: 774975, episodes: 31000, mean episode reward: -24.974335236384576, agent episode reward: [-21.32745405345806, -1.8234405914632565, -1.8234405914632565], time: 107.259
steps: 799975, episodes: 32000, mean episode reward: -23.699635491802955, agent episode reward: [-21.469684401674705, -1.1149755450641228, -1.1149755450641228], time: 106.982
steps: 824975, episodes: 33000, mean episode reward: -23.593979536346893, agent episode reward: [-21.554877195181064, -1.0195511705829166, -1.0195511705829166], time: 107.212
steps: 849975, episodes: 34000, mean episode reward: -23.024284908333538, agent episode reward: [-21.946604818924325, -0.5388400447046077, -0.5388400447046077], time: 107.724
steps: 874975, episodes: 35000, mean episode reward: -25.00463487389576, agent episode reward: [-21.469440114857377, -1.7675973795191913, -1.7675973795191913], time: 112.284
steps: 899975, episodes: 36000, mean episode reward: -24.398725340380363, agent episode reward: [-21.364139268748833, -1.517293035815766, -1.517293035815766], time: 122.342
steps: 924975, episodes: 37000, mean episode reward: -24.071366105376814, agent episode reward: [-21.96899928947486, -1.0511834079509785, -1.0511834079509785], time: 122.702
steps: 949975, episodes: 38000, mean episode reward: -25.206434127741122, agent episode reward: [-22.164366492196166, -1.5210338177724774, -1.5210338177724774], time: 122.16
steps: 974975, episodes: 39000, mean episode reward: -23.932720166535837, agent episode reward: [-22.398906698555408, -0.7669067339902128, -0.7669067339902128], time: 121.659
steps: 999975, episodes: 40000, mean episode reward: -25.10190771161452, agent episode reward: [-21.867530492110266, -1.6171886097521275, -1.6171886097521275], time: 122.577
steps: 1024975, episodes: 41000, mean episode reward: -23.771463853366292, agent episode reward: [-22.51626424787112, -0.6275998027475834, -0.6275998027475834], time: 122.993
steps: 1049975, episodes: 42000, mean episode reward: -25.099200299655127, agent episode reward: [-22.002936208468032, -1.548132045593547, -1.548132045593547], time: 122.353
steps: 1074975, episodes: 43000, mean episode reward: -25.34059931002284, agent episode reward: [-21.876637283019132, -1.7319810135018534, -1.7319810135018534], time: 122.172
steps: 1099975, episodes: 44000, mean episode reward: -25.39534647221201, agent episode reward: [-22.07840815872362, -1.6584691567441987, -1.6584691567441987], time: 122.436
steps: 1124975, episodes: 45000, mean episode reward: -25.277194703232308, agent episode reward: [-21.702000683909972, -1.7875970096611682, -1.7875970096611682], time: 124.251
steps: 1149975, episodes: 46000, mean episode reward: -24.43698078502039, agent episode reward: [-22.23983455347815, -1.0985731157711178, -1.0985731157711178], time: 123.1
steps: 1174975, episodes: 47000, mean episode reward: -24.97637895193966, agent episode reward: [-21.84176814466616, -1.5673054036367526, -1.5673054036367526], time: 122.304
steps: 1199975, episodes: 48000, mean episode reward: -24.378433914262015, agent episode reward: [-21.72144923493299, -1.3284923396645132, -1.3284923396645132], time: 122.005
steps: 1224975, episodes: 49000, mean episode reward: -23.70478723094541, agent episode reward: [-22.250718941685836, -0.7270341446297873, -0.7270341446297873], time: 122.26
steps: 1249975, episodes: 50000, mean episode reward: -24.597832317895225, agent episode reward: [-21.740150380862698, -1.4288409685162642, -1.4288409685162642], time: 122.262
steps: 1274975, episodes: 51000, mean episode reward: -25.28799378417635, agent episode reward: [-21.993219319960247, -1.64738723210805, -1.64738723210805], time: 120.7
steps: 1299975, episodes: 52000, mean episode reward: -24.92971188150499, agent episode reward: [-22.195851364229007, -1.3669302586379906, -1.3669302586379906], time: 122.122
steps: 1324975, episodes: 53000, mean episode reward: -25.449432341406858, agent episode reward: [-22.004331246295962, -1.7225505475554492, -1.7225505475554492], time: 122.607
steps: 1349975, episodes: 54000, mean episode reward: -23.92178210521507, agent episode reward: [-22.352820420970115, -0.7844808421224762, -0.7844808421224762], time: 121.731
steps: 1374975, episodes: 55000, mean episode reward: -24.864385432438745, agent episode reward: [-22.238891281730208, -1.3127470753542663, -1.3127470753542663], time: 122.845
steps: 1399975, episodes: 56000, mean episode reward: -25.001413235350913, agent episode reward: [-22.531839307690284, -1.2347869638303155, -1.2347869638303155], time: 122.117
steps: 1424975, episodes: 57000, mean episode reward: -24.25207578322171, agent episode reward: [-22.814756228935643, -0.7186597771430341, -0.7186597771430341], time: 123.05
steps: 1449975, episodes: 58000, mean episode reward: -25.73223543106707, agent episode reward: [-22.459061064420645, -1.6365871833232122, -1.6365871833232122], time: 122.285
steps: 1474975, episodes: 59000, mean episode reward: -25.537898792332086, agent episode reward: [-22.380557015668735, -1.5786708883316765, -1.5786708883316765], time: 122.332
steps: 1499975, episodes: 60000, mean episode reward: -25.870926097241384, agent episode reward: [-22.475935502959242, -1.6974952971410693, -1.6974952971410693], time: 121.611
...Finished total of 60001 episodes.
