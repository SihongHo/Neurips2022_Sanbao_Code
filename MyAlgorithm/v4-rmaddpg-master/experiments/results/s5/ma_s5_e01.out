----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------sent
env.action_space is [Discrete(4), Discrete(4), Discrete(4)] 
env.observation_space is [Box(4,), Box(8,), Box(8,)] 
obs_shape_n is [(4,), (8,), (8,)] 
Using good policy maddpg and adv policy maddpg
[Discrete(4), Discrete(8), Discrete(8)]
Using noise policy maddpg
There is 3 adversaries
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -21.13374603355054, agent episode reward: [-23.270935680313446, 1.0685948233814522, 1.0685948233814522], time: 95.509
steps: 49975, episodes: 2000, mean episode reward: -18.354688463189383, agent episode reward: [-18.502919788449955, 0.07411566263028456, 0.07411566263028456], time: 136.356
steps: 74975, episodes: 3000, mean episode reward: -17.128422763357975, agent episode reward: [-17.92480821037398, 0.39819272350800283, 0.39819272350800283], time: 124.137
steps: 99975, episodes: 4000, mean episode reward: -18.09603336064482, agent episode reward: [-19.814009800575498, 0.8589882199653383, 0.8589882199653383], time: 115.094
steps: 124975, episodes: 5000, mean episode reward: -17.837938779821112, agent episode reward: [-19.12772804640713, 0.6448946332930078, 0.6448946332930078], time: 134.748
steps: 149975, episodes: 6000, mean episode reward: -17.683703843691095, agent episode reward: [-18.047685361335667, 0.1819907588222871, 0.1819907588222871], time: 130.974
steps: 174975, episodes: 7000, mean episode reward: -18.22949150948681, agent episode reward: [-18.112218798419605, -0.05863635553359912, -0.05863635553359912], time: 150.696
steps: 199975, episodes: 8000, mean episode reward: -19.98302195605057, agent episode reward: [-18.322709829754157, -0.830156063148208, -0.830156063148208], time: 136.69
steps: 224975, episodes: 9000, mean episode reward: -22.324691940260987, agent episode reward: [-19.110658969002852, -1.6070164856290658, -1.6070164856290658], time: 138.42
steps: 249975, episodes: 10000, mean episode reward: -24.762694261415522, agent episode reward: [-19.24208735051765, -2.760303455448938, -2.760303455448938], time: 145.047
steps: 274975, episodes: 11000, mean episode reward: -23.35432299286147, agent episode reward: [-19.315267041656515, -2.0195279756024758, -2.0195279756024758], time: 133.262
steps: 299975, episodes: 12000, mean episode reward: -22.909630114336558, agent episode reward: [-19.94563643399119, -1.4819968401726844, -1.4819968401726844], time: 122.801
steps: 324975, episodes: 13000, mean episode reward: -24.86498930327386, agent episode reward: [-20.593774007262414, -2.1356076480057227, -2.1356076480057227], time: 152.478
steps: 349975, episodes: 14000, mean episode reward: -25.12615665612648, agent episode reward: [-21.533508934531753, -1.7963238607973657, -1.7963238607973657], time: 125.611
steps: 374975, episodes: 15000, mean episode reward: -25.047801991880753, agent episode reward: [-22.09713247821202, -1.4753347568343649, -1.4753347568343649], time: 125.732
steps: 399975, episodes: 16000, mean episode reward: -22.63393967336946, agent episode reward: [-20.589303852603095, -1.0223179103831868, -1.0223179103831868], time: 104.349
steps: 424975, episodes: 17000, mean episode reward: -25.22398589149388, agent episode reward: [-22.427751206481588, -1.3981173425061448, -1.3981173425061448], time: 119.248
steps: 449975, episodes: 18000, mean episode reward: -24.216214614880258, agent episode reward: [-21.361470293366533, -1.4273721607568628, -1.4273721607568628], time: 108.559
steps: 474975, episodes: 19000, mean episode reward: -23.305707738064985, agent episode reward: [-21.0003719880628, -1.152667875001093, -1.152667875001093], time: 97.353
steps: 499975, episodes: 20000, mean episode reward: -24.35327308310593, agent episode reward: [-21.73974869819072, -1.3067621924576047, -1.3067621924576047], time: 96.084
steps: 524975, episodes: 21000, mean episode reward: -24.080456525956457, agent episode reward: [-21.80034444880049, -1.140056038577982, -1.140056038577982], time: 118.948
steps: 549975, episodes: 22000, mean episode reward: -24.531365709970856, agent episode reward: [-22.298626330286623, -1.1163696898421172, -1.1163696898421172], time: 100.414
steps: 574975, episodes: 23000, mean episode reward: -23.36679638294897, agent episode reward: [-21.035053434610745, -1.1658714741691134, -1.1658714741691134], time: 99.553
steps: 599975, episodes: 24000, mean episode reward: -23.63665955529375, agent episode reward: [-21.877521204698052, -0.8795691752978494, -0.8795691752978494], time: 122.168
steps: 624975, episodes: 25000, mean episode reward: -23.814149465360593, agent episode reward: [-22.091890819095912, -0.8611293231323379, -0.8611293231323379], time: 116.67
steps: 649975, episodes: 26000, mean episode reward: -25.056718152082198, agent episode reward: [-22.468972664456746, -1.293872743812727, -1.293872743812727], time: 110.904
steps: 674975, episodes: 27000, mean episode reward: -23.46362983631943, agent episode reward: [-21.960022159021186, -0.7518038386491184, -0.7518038386491184], time: 130.589
steps: 699975, episodes: 28000, mean episode reward: -23.081182041368827, agent episode reward: [-21.37509945900154, -0.8530412911836436, -0.8530412911836436], time: 125.23
steps: 724975, episodes: 29000, mean episode reward: -24.507765154603923, agent episode reward: [-22.833625303731427, -0.8370699254362478, -0.8370699254362478], time: 108.797
steps: 749975, episodes: 30000, mean episode reward: -23.5493143035058, agent episode reward: [-22.92246859304296, -0.31342285523141733, -0.31342285523141733], time: 101.308
steps: 774975, episodes: 31000, mean episode reward: -22.695483076940047, agent episode reward: [-22.875134528578187, 0.08982572581907021, 0.08982572581907021], time: 110.214
steps: 799975, episodes: 32000, mean episode reward: -23.37277106455885, agent episode reward: [-23.426106296181505, 0.026667615811325006, 0.026667615811325006], time: 110.474
steps: 824975, episodes: 33000, mean episode reward: -23.433593815940185, agent episode reward: [-23.327373095105933, -0.053110360417126086, -0.053110360417126086], time: 106.988
steps: 849975, episodes: 34000, mean episode reward: -23.268861345622277, agent episode reward: [-23.054549096933837, -0.10715612434422216, -0.10715612434422216], time: 115.919
steps: 874975, episodes: 35000, mean episode reward: -23.471687505767317, agent episode reward: [-23.476624835098338, 0.002468664665512307, 0.002468664665512307], time: 122.8
steps: 899975, episodes: 36000, mean episode reward: -23.290255429972387, agent episode reward: [-23.57769866853291, 0.14372161928026275, 0.14372161928026275], time: 115.633
steps: 924975, episodes: 37000, mean episode reward: -23.052647348917642, agent episode reward: [-23.071089148033792, 0.009220899558074052, 0.009220899558074052], time: 110.51
steps: 949975, episodes: 38000, mean episode reward: -22.482021128169286, agent episode reward: [-22.378642707174084, -0.05168921049760157, -0.05168921049760157], time: 109.401
steps: 974975, episodes: 39000, mean episode reward: -22.760171671444283, agent episode reward: [-22.760166617332047, -2.5270561174366437e-06, -2.5270561174366437e-06], time: 121.796
steps: 999975, episodes: 40000, mean episode reward: -23.83732157619504, agent episode reward: [-24.30554760707061, 0.23411301543778523, 0.23411301543778523], time: 107.299
steps: 1024975, episodes: 41000, mean episode reward: -23.128172997697344, agent episode reward: [-23.631466089936225, 0.25164654611943954, 0.25164654611943954], time: 107.579
steps: 1049975, episodes: 42000, mean episode reward: -22.9067354114337, agent episode reward: [-23.258371495646276, 0.17581804210628865, 0.17581804210628865], time: 109.643
steps: 1074975, episodes: 43000, mean episode reward: -22.371176758426408, agent episode reward: [-22.979214112833837, 0.3040186772037133, 0.3040186772037133], time: 104.531
steps: 1099975, episodes: 44000, mean episode reward: -22.872626693387495, agent episode reward: [-23.19493952194957, 0.16115641428103958, 0.16115641428103958], time: 92.643
steps: 1124975, episodes: 45000, mean episode reward: -22.260433260743607, agent episode reward: [-23.94648737489839, 0.8430270570773911, 0.8430270570773911], time: 97.303
steps: 1149975, episodes: 46000, mean episode reward: -23.692115510034483, agent episode reward: [-24.92477038704803, 0.616327438506773, 0.616327438506773], time: 91.163
steps: 1174975, episodes: 47000, mean episode reward: -23.00643707199862, agent episode reward: [-23.81788315227597, 0.4057230401386762, 0.4057230401386762], time: 113.068
steps: 1199975, episodes: 48000, mean episode reward: -22.681580064235213, agent episode reward: [-22.921627773694453, 0.12002385472961943, 0.12002385472961943], time: 117.618
steps: 1224975, episodes: 49000, mean episode reward: -23.18724379543956, agent episode reward: [-23.75557437849302, 0.2841652915267303, 0.2841652915267303], time: 122.251
steps: 1249975, episodes: 50000, mean episode reward: -22.763010340355688, agent episode reward: [-23.278331458376176, 0.25766055901024126, 0.25766055901024126], time: 114.769
steps: 1274975, episodes: 51000, mean episode reward: -23.04281868957771, agent episode reward: [-23.880938196504612, 0.41905975346344904, 0.41905975346344904], time: 111.017
steps: 1299975, episodes: 52000, mean episode reward: -23.270660459026374, agent episode reward: [-22.91812939349657, -0.1762655327649001, -0.1762655327649001], time: 111.373
steps: 1324975, episodes: 53000, mean episode reward: -23.632194264646554, agent episode reward: [-23.578565198673584, -0.02681453298648576, -0.02681453298648576], time: 112.415
steps: 1349975, episodes: 54000, mean episode reward: -24.01118906768449, agent episode reward: [-24.498424335328483, 0.2436176338219945, 0.2436176338219945], time: 116.497
steps: 1374975, episodes: 55000, mean episode reward: -23.020781426001776, agent episode reward: [-22.801904634532995, -0.10943839573438888, -0.10943839573438888], time: 117.576
steps: 1399975, episodes: 56000, mean episode reward: -23.43270511922309, agent episode reward: [-22.738652977664916, -0.3470260707790883, -0.3470260707790883], time: 116.557
steps: 1424975, episodes: 57000, mean episode reward: -25.02004090840077, agent episode reward: [-24.911781221355515, -0.054129843522629374, -0.054129843522629374], time: 117.52
steps: 1449975, episodes: 58000, mean episode reward: -22.185086418063086, agent episode reward: [-22.550584871711887, 0.1827492268244002, 0.1827492268244002], time: 116.712
steps: 1474975, episodes: 59000, mean episode reward: -24.020059964233464, agent episode reward: [-23.960224068819908, -0.02991794770677727, -0.02991794770677727], time: 114.433
steps: 1499975, episodes: 60000, mean episode reward: -23.67884515480553, agent episode reward: [-23.84045324855766, 0.080804046876064, 0.080804046876064], time: 117.127
...Finished total of 60001 episodes.
----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------end
