----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------sent
env.action_space is [Discrete(4), Discrete(4), Discrete(4)] 
env.observation_space is [Box(4,), Box(8,), Box(8,)] 
obs_shape_n is [(4,), (8,), (8,)] 
Using good policy maddpg and adv policy maddpg
[Discrete(4), Discrete(8), Discrete(8)]
Using noise policy maddpg
There is 3 adversaries
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.038746047448612, agent episode reward: [-24.49828654376293, 0.7297702481571596, 0.7297702481571596], time: 246.453
steps: 49975, episodes: 2000, mean episode reward: -17.963661446835925, agent episode reward: [-18.842290621034575, 0.4393145870993252, 0.4393145870993252], time: 397.416
steps: 74975, episodes: 3000, mean episode reward: -16.5410945582375, agent episode reward: [-18.74897747195963, 1.1039414568610646, 1.1039414568610646], time: 396.367
steps: 99975, episodes: 4000, mean episode reward: -20.33747180351912, agent episode reward: [-20.815301257485334, 0.2389147269831083, 0.2389147269831083], time: 393.643
steps: 124975, episodes: 5000, mean episode reward: -21.481834157504586, agent episode reward: [-21.723374210963776, 0.12077002672959518, 0.12077002672959518], time: 395.264
steps: 149975, episodes: 6000, mean episode reward: -23.94551596142106, agent episode reward: [-21.895783344700117, -1.0248663083604688, -1.0248663083604688], time: 393.381
steps: 174975, episodes: 7000, mean episode reward: -24.483199756871073, agent episode reward: [-22.320118329099802, -1.0815407138856372, -1.0815407138856372], time: 396.647
steps: 199975, episodes: 8000, mean episode reward: -25.250347611098583, agent episode reward: [-22.97167539610216, -1.1393361074982133, -1.1393361074982133], time: 396.312
steps: 224975, episodes: 9000, mean episode reward: -23.914631110880357, agent episode reward: [-23.425042354501993, -0.2447943781891807, -0.2447943781891807], time: 398.93
steps: 249975, episodes: 10000, mean episode reward: -23.335559736795847, agent episode reward: [-23.02078036006353, -0.15738968836615572, -0.15738968836615572], time: 489.077
steps: 274975, episodes: 11000, mean episode reward: -22.150268589959175, agent episode reward: [-23.248524259704702, 0.5491278348727637, 0.5491278348727637], time: 522.019
steps: 299975, episodes: 12000, mean episode reward: -25.285159783662063, agent episode reward: [-23.976529553885374, -0.6543151148883455, -0.6543151148883455], time: 396.252
steps: 324975, episodes: 13000, mean episode reward: -24.476852404686205, agent episode reward: [-23.967681863642035, -0.2545852705220841, -0.2545852705220841], time: 399.123
steps: 349975, episodes: 14000, mean episode reward: -24.656390412342624, agent episode reward: [-23.446342825335524, -0.605023793503551, -0.605023793503551], time: 399.235
steps: 374975, episodes: 15000, mean episode reward: -24.830433108522055, agent episode reward: [-23.260571701456705, -0.7849307035326715, -0.7849307035326715], time: 401.399
steps: 399975, episodes: 16000, mean episode reward: -25.71876567687407, agent episode reward: [-23.841120423127926, -0.9388226268730742, -0.9388226268730742], time: 397.338
steps: 424975, episodes: 17000, mean episode reward: -24.27547627388922, agent episode reward: [-22.708687600983342, -0.7833943364529359, -0.7833943364529359], time: 398.295
steps: 449975, episodes: 18000, mean episode reward: -26.535459034833153, agent episode reward: [-23.71400217862627, -1.4107284281034391, -1.4107284281034391], time: 410.962
steps: 474975, episodes: 19000, mean episode reward: -25.54912303746494, agent episode reward: [-23.56596194556527, -0.9915805459498341, -0.9915805459498341], time: 584.076
steps: 499975, episodes: 20000, mean episode reward: -24.36796548366604, agent episode reward: [-23.016275812374886, -0.6758448356455788, -0.6758448356455788], time: 594.834
steps: 524975, episodes: 21000, mean episode reward: -24.33699270680073, agent episode reward: [-23.33116875992502, -0.5029119734378537, -0.5029119734378537], time: 597.389
steps: 549975, episodes: 22000, mean episode reward: -25.78270134803823, agent episode reward: [-23.682814244883865, -1.0499435515771804, -1.0499435515771804], time: 600.331
steps: 574975, episodes: 23000, mean episode reward: -27.347310683668102, agent episode reward: [-23.727429767181096, -1.8099404582435026, -1.8099404582435026], time: 600.022
steps: 599975, episodes: 24000, mean episode reward: -25.13964706464574, agent episode reward: [-23.724594099456567, -0.707526482594586, -0.707526482594586], time: 597.459
steps: 624975, episodes: 25000, mean episode reward: -24.47000845515066, agent episode reward: [-23.65574889275121, -0.40712978119972226, -0.40712978119972226], time: 851.461
steps: 649975, episodes: 26000, mean episode reward: -24.70742191809056, agent episode reward: [-23.355335278522933, -0.6760433197838146, -0.6760433197838146], time: 844.462
steps: 674975, episodes: 27000, mean episode reward: -24.773062580682787, agent episode reward: [-23.73672800728937, -0.5181672866967103, -0.5181672866967103], time: 840.248
steps: 699975, episodes: 28000, mean episode reward: -25.21208541187399, agent episode reward: [-24.03536267054404, -0.5883613706649755, -0.5883613706649755], time: 840.173
steps: 724975, episodes: 29000, mean episode reward: -26.00511417635072, agent episode reward: [-24.06721812333639, -0.9689480265071633, -0.9689480265071633], time: 844.479
steps: 749975, episodes: 30000, mean episode reward: -24.482823372640052, agent episode reward: [-23.81539083493996, -0.3337162688500437, -0.3337162688500437], time: 839.762
steps: 774975, episodes: 31000, mean episode reward: -25.21597232331249, agent episode reward: [-24.089748361245714, -0.5631119810333892, -0.5631119810333892], time: 840.913
steps: 799975, episodes: 32000, mean episode reward: -25.090212050168667, agent episode reward: [-23.986003573537868, -0.5521042383153968, -0.5521042383153968], time: 842.524
steps: 824975, episodes: 33000, mean episode reward: -23.838640057374366, agent episode reward: [-24.182264232356662, 0.17181208749114923, 0.17181208749114923], time: 841.413
steps: 849975, episodes: 34000, mean episode reward: -24.884157649921814, agent episode reward: [-24.3401018282913, -0.2720279108152567, -0.2720279108152567], time: 844.445
steps: 874975, episodes: 35000, mean episode reward: -25.858244209423766, agent episode reward: [-24.62723494807377, -0.6155046306749994, -0.6155046306749994], time: 836.036
steps: 899975, episodes: 36000, mean episode reward: -25.48370561701318, agent episode reward: [-24.5008956274536, -0.49140499477978883, -0.49140499477978883], time: 846.93
steps: 924975, episodes: 37000, mean episode reward: -24.702264681235075, agent episode reward: [-24.49961650368739, -0.1013240887738439, -0.1013240887738439], time: 844.55
steps: 949975, episodes: 38000, mean episode reward: -25.05679140452234, agent episode reward: [-24.172691079199346, -0.4420501626614973, -0.4420501626614973], time: 840.718
steps: 974975, episodes: 39000, mean episode reward: -25.421651849454197, agent episode reward: [-24.522222245477796, -0.4497148019882014, -0.4497148019882014], time: 844.18
steps: 999975, episodes: 40000, mean episode reward: -24.736591964836503, agent episode reward: [-24.280731846986367, -0.22793005892506857, -0.22793005892506857], time: 838.709
steps: 1024975, episodes: 41000, mean episode reward: -24.311615492569768, agent episode reward: [-24.15147229494588, -0.08007159881194179, -0.08007159881194179], time: 846.048
steps: 1049975, episodes: 42000, mean episode reward: -23.256849869141217, agent episode reward: [-24.135592872886164, 0.4393715018724741, 0.4393715018724741], time: 845.628
steps: 1074975, episodes: 43000, mean episode reward: -25.049653808761523, agent episode reward: [-24.511945040791925, -0.26885438398480027, -0.26885438398480027], time: 849.294
steps: 1099975, episodes: 44000, mean episode reward: -23.43657531293863, agent episode reward: [-24.284631118250417, 0.424027902655895, 0.424027902655895], time: 803.66
steps: 1124975, episodes: 45000, mean episode reward: -25.83974092532329, agent episode reward: [-25.179262457141462, -0.3302392340909132, -0.3302392340909132], time: 623.774
steps: 1149975, episodes: 46000, mean episode reward: -23.569825327879933, agent episode reward: [-24.14002762598597, 0.2851011490530192, 0.2851011490530192], time: 625.462
steps: 1174975, episodes: 47000, mean episode reward: -23.674622180420442, agent episode reward: [-24.288469636784765, 0.30692372818215946, 0.30692372818215946], time: 606.006
steps: 1199975, episodes: 48000, mean episode reward: -23.892622309778236, agent episode reward: [-24.050792944647082, 0.07908531743442423, 0.07908531743442423], time: 394.065
steps: 1224975, episodes: 49000, mean episode reward: -23.280589983656085, agent episode reward: [-24.06877909512217, 0.3940945557330423, 0.3940945557330423], time: 393.004
steps: 1249975, episodes: 50000, mean episode reward: -24.30326197640591, agent episode reward: [-24.466455481791076, 0.0815967526925825, 0.0815967526925825], time: 392.702
steps: 1274975, episodes: 51000, mean episode reward: -24.467517476327444, agent episode reward: [-24.276503113595567, -0.09550718136593883, -0.09550718136593883], time: 394.563
steps: 1299975, episodes: 52000, mean episode reward: -24.153699900130675, agent episode reward: [-24.72245685433222, 0.28437847710077063, 0.28437847710077063], time: 393.533
steps: 1324975, episodes: 53000, mean episode reward: -24.915954229525667, agent episode reward: [-25.323677260460173, 0.20386151546725015, 0.20386151546725015], time: 395.928
steps: 1349975, episodes: 54000, mean episode reward: -25.13913495380342, agent episode reward: [-25.101550453702234, -0.018792250050591135, -0.018792250050591135], time: 391.718
steps: 1374975, episodes: 55000, mean episode reward: -25.515378279807113, agent episode reward: [-24.79011705687032, -0.36263061146839626, -0.36263061146839626], time: 394.182
steps: 1399975, episodes: 56000, mean episode reward: -24.434652679897923, agent episode reward: [-25.11474198673889, 0.34004465342048207, 0.34004465342048207], time: 394.089
steps: 1424975, episodes: 57000, mean episode reward: -23.159755291286938, agent episode reward: [-24.49019120672664, 0.6652179577198478, 0.6652179577198478], time: 394.695
steps: 1449975, episodes: 58000, mean episode reward: -23.30379661637528, agent episode reward: [-24.658018562909554, 0.677110973267137, 0.677110973267137], time: 393.415
steps: 1474975, episodes: 59000, mean episode reward: -22.63365689561912, agent episode reward: [-24.104877310827455, 0.7356102076041695, 0.7356102076041695], time: 393.665
steps: 1499975, episodes: 60000, mean episode reward: -23.16648954689587, agent episode reward: [-24.571781469323586, 0.7026459612138576, 0.7026459612138576], time: 396.011
...Finished total of 60001 episodes.
----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------end
