----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------sent
env.action_space is [Discrete(4), Discrete(4), Discrete(4)] 
env.observation_space is [Box(4,), Box(8,), Box(8,)] 
obs_shape_n is [(4,), (8,), (8,)] 
Using good policy maddpg and adv policy maddpg
[Discrete(4), Discrete(8), Discrete(8)]
Using noise policy maddpg
There is 3 adversaries
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -21.66653447210316, agent episode reward: [-25.62036892018858, 1.9769172240427106, 1.9769172240427106], time: 64.555
steps: 49975, episodes: 2000, mean episode reward: -19.453960823307323, agent episode reward: [-18.945349757695215, -0.2543055328060575, -0.2543055328060575], time: 117.656
steps: 74975, episodes: 3000, mean episode reward: -19.89832980480881, agent episode reward: [-20.886916968627027, 0.4942935819091076, 0.4942935819091076], time: 113.167
steps: 99975, episodes: 4000, mean episode reward: -19.579534351353338, agent episode reward: [-22.593851949969174, 1.507158799307918, 1.507158799307918], time: 97.071
steps: 124975, episodes: 5000, mean episode reward: -20.71310500588265, agent episode reward: [-23.185128838634967, 1.2360119163761578, 1.2360119163761578], time: 96.527
steps: 149975, episodes: 6000, mean episode reward: -22.226049440762473, agent episode reward: [-22.06746555525834, -0.07929194275206422, -0.07929194275206422], time: 96.251
steps: 174975, episodes: 7000, mean episode reward: -24.36623148117526, agent episode reward: [-24.614868601594047, 0.12431856020939426, 0.12431856020939426], time: 95.173
steps: 199975, episodes: 8000, mean episode reward: -24.26480309789192, agent episode reward: [-25.229737206759964, 0.48246705443402155, 0.48246705443402155], time: 97.109
steps: 224975, episodes: 9000, mean episode reward: -24.127852318198645, agent episode reward: [-24.735168793048274, 0.30365823742481435, 0.30365823742481435], time: 103.328
steps: 249975, episodes: 10000, mean episode reward: -24.063006799468734, agent episode reward: [-24.487777405586336, 0.2123853030587986, 0.2123853030587986], time: 95.763
steps: 274975, episodes: 11000, mean episode reward: -24.189627225983852, agent episode reward: [-23.460633896977868, -0.3644966645029926, -0.3644966645029926], time: 95.247
steps: 299975, episodes: 12000, mean episode reward: -23.675717358207113, agent episode reward: [-24.065037540283402, 0.19466009103814672, 0.19466009103814672], time: 95.21
steps: 324975, episodes: 13000, mean episode reward: -23.80686153919966, agent episode reward: [-24.53340883987792, 0.36327365033913045, 0.36327365033913045], time: 96.622
steps: 349975, episodes: 14000, mean episode reward: -24.846782996009008, agent episode reward: [-24.3134513060263, -0.2666658449913565, -0.2666658449913565], time: 101.122
steps: 374975, episodes: 15000, mean episode reward: -24.874303358381717, agent episode reward: [-24.262364428990978, -0.3059694646953693, -0.3059694646953693], time: 114.978
steps: 399975, episodes: 16000, mean episode reward: -24.213913037119113, agent episode reward: [-24.601543970881956, 0.19381546688142298, 0.19381546688142298], time: 114.431
steps: 424975, episodes: 17000, mean episode reward: -24.58908788703559, agent episode reward: [-24.586897509761634, -0.0010951886369761112, -0.0010951886369761112], time: 129.41
steps: 449975, episodes: 18000, mean episode reward: -24.904006309035886, agent episode reward: [-24.334621554388466, -0.28469237732371006, -0.28469237732371006], time: 127.167
steps: 474975, episodes: 19000, mean episode reward: -23.97175683333349, agent episode reward: [-24.319548030897998, 0.17389559878225408, 0.17389559878225408], time: 121.021
steps: 499975, episodes: 20000, mean episode reward: -23.923042864772174, agent episode reward: [-24.262615495292184, 0.16978631526000487, 0.16978631526000487], time: 112.098
steps: 524975, episodes: 21000, mean episode reward: -23.450715944413737, agent episode reward: [-23.85460443888047, 0.20194424723336765, 0.20194424723336765], time: 123.305
steps: 549975, episodes: 22000, mean episode reward: -23.866506913323967, agent episode reward: [-24.155187088941364, 0.1443400878087001, 0.1443400878087001], time: 102.951
steps: 574975, episodes: 23000, mean episode reward: -23.984103668902833, agent episode reward: [-24.049761468792386, 0.03282889994477591, 0.03282889994477591], time: 94.24
steps: 599975, episodes: 24000, mean episode reward: -24.38235431242774, agent episode reward: [-24.128150379943364, -0.12710196624218772, -0.12710196624218772], time: 90.199
steps: 624975, episodes: 25000, mean episode reward: -23.416354318011, agent episode reward: [-24.60689591903968, 0.5952708005143402, 0.5952708005143402], time: 92.843
steps: 649975, episodes: 26000, mean episode reward: -23.760785607805126, agent episode reward: [-24.472206699585524, 0.35571054589019857, 0.35571054589019857], time: 107.533
steps: 674975, episodes: 27000, mean episode reward: -23.960503016826294, agent episode reward: [-24.0898927696432, 0.06469487640845331, 0.06469487640845331], time: 103.043
steps: 699975, episodes: 28000, mean episode reward: -24.023292547329312, agent episode reward: [-24.808720149169233, 0.39271380091996044, 0.39271380091996044], time: 95.349
steps: 724975, episodes: 29000, mean episode reward: -23.971996915051236, agent episode reward: [-24.69316792086712, 0.36058550290794195, 0.36058550290794195], time: 101.634
steps: 749975, episodes: 30000, mean episode reward: -23.235458224887143, agent episode reward: [-24.308203899268047, 0.5363728371904508, 0.5363728371904508], time: 119.847
steps: 774975, episodes: 31000, mean episode reward: -24.10737529312754, agent episode reward: [-24.62868680023832, 0.2606557535553891, 0.2606557535553891], time: 122.631
steps: 799975, episodes: 32000, mean episode reward: -24.711987177712405, agent episode reward: [-24.872139930323844, 0.08007637630571994, 0.08007637630571994], time: 110.641
steps: 824975, episodes: 33000, mean episode reward: -24.4714253038842, agent episode reward: [-25.029234895135392, 0.27890479562559745, 0.27890479562559745], time: 101.543
steps: 849975, episodes: 34000, mean episode reward: -24.02931466645805, agent episode reward: [-24.804865334963434, 0.3877753342526927, 0.3877753342526927], time: 106.129
steps: 874975, episodes: 35000, mean episode reward: -23.391076635788757, agent episode reward: [-25.168821783681675, 0.8888725739464616, 0.8888725739464616], time: 96.132
steps: 899975, episodes: 36000, mean episode reward: -24.382051068677605, agent episode reward: [-25.190546017776068, 0.40424747454923327, 0.40424747454923327], time: 99.986
steps: 924975, episodes: 37000, mean episode reward: -23.24988223901809, agent episode reward: [-24.761995807989784, 0.7560567844858488, 0.7560567844858488], time: 117.783
steps: 949975, episodes: 38000, mean episode reward: -23.387081267060253, agent episode reward: [-25.25654335844557, 0.934731045692656, 0.934731045692656], time: 107.011
steps: 974975, episodes: 39000, mean episode reward: -24.287148582193403, agent episode reward: [-25.320719177598107, 0.5167852977023528, 0.5167852977023528], time: 111.699
steps: 999975, episodes: 40000, mean episode reward: -23.44487311284088, agent episode reward: [-25.444088654663815, 0.9996077709114662, 0.9996077709114662], time: 141.018
steps: 1024975, episodes: 41000, mean episode reward: -23.699841628768187, agent episode reward: [-25.16798120721318, 0.7340697892224987, 0.7340697892224987], time: 193.069
steps: 1049975, episodes: 42000, mean episode reward: -24.44050435449439, agent episode reward: [-25.72456373127541, 0.6420296883905101, 0.6420296883905101], time: 129.298
steps: 1074975, episodes: 43000, mean episode reward: -23.787836595390054, agent episode reward: [-24.895741964065113, 0.5539526843375291, 0.5539526843375291], time: 123.022
steps: 1099975, episodes: 44000, mean episode reward: -23.927083432377085, agent episode reward: [-25.40644680332568, 0.7396816854742971, 0.7396816854742971], time: 131.518
steps: 1124975, episodes: 45000, mean episode reward: -24.26970653845019, agent episode reward: [-24.892635598265365, 0.31146452990758655, 0.31146452990758655], time: 135.39
steps: 1149975, episodes: 46000, mean episode reward: -25.681202064363454, agent episode reward: [-24.99991988437973, -0.340641089991862, -0.340641089991862], time: 126.543
steps: 1174975, episodes: 47000, mean episode reward: -24.938856219095747, agent episode reward: [-25.18488385197959, 0.12301381644192307, 0.12301381644192307], time: 112.274
steps: 1199975, episodes: 48000, mean episode reward: -23.5220851457007, agent episode reward: [-24.879979080929658, 0.6789469676144784, 0.6789469676144784], time: 112.851
steps: 1224975, episodes: 49000, mean episode reward: -23.821788067503796, agent episode reward: [-25.016928574629713, 0.5975702535629598, 0.5975702535629598], time: 114.965
steps: 1249975, episodes: 50000, mean episode reward: -23.731162459203375, agent episode reward: [-24.71596804960675, 0.4924027952016861, 0.4924027952016861], time: 119.813
steps: 1274975, episodes: 51000, mean episode reward: -25.14063848893091, agent episode reward: [-25.20308340410635, 0.031222457587718508, 0.031222457587718508], time: 111.746
steps: 1299975, episodes: 52000, mean episode reward: -24.315682245784767, agent episode reward: [-24.872772592286907, 0.2785451732510681, 0.2785451732510681], time: 120.501
steps: 1324975, episodes: 53000, mean episode reward: -26.381528205627635, agent episode reward: [-24.9430499663366, -0.719239119645517, -0.719239119645517], time: 106.914
steps: 1349975, episodes: 54000, mean episode reward: -26.529465397920074, agent episode reward: [-25.181800560859447, -0.6738324185303148, -0.6738324185303148], time: 114.798
steps: 1374975, episodes: 55000, mean episode reward: -27.197270690931703, agent episode reward: [-25.23722774626937, -0.9800214723311677, -0.9800214723311677], time: 147.415
steps: 1399975, episodes: 56000, mean episode reward: -27.222182911346586, agent episode reward: [-24.767499756785263, -1.2273415772806604, -1.2273415772806604], time: 133.49
steps: 1424975, episodes: 57000, mean episode reward: -27.95511479257741, agent episode reward: [-25.128699006721035, -1.4132078929281888, -1.4132078929281888], time: 118.766
steps: 1449975, episodes: 58000, mean episode reward: -27.696280742202482, agent episode reward: [-25.2504889436338, -1.2228958992843404, -1.2228958992843404], time: 130.399
steps: 1474975, episodes: 59000, mean episode reward: -27.175976486126913, agent episode reward: [-24.868234588095454, -1.1538709490157295, -1.1538709490157295], time: 128.314
steps: 1499975, episodes: 60000, mean episode reward: -26.36776273579883, agent episode reward: [-24.672322063188822, -0.847720336305004, -0.847720336305004], time: 124.49
...Finished total of 60001 episodes.
----------------------------------2
----------------------------------3
----------------------------------4
----------------------------------5
----------------------------------6
----------------------------------end
